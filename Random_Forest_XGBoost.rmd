---
title: "Random Forest Parameter Tuning"
author: "Chamroeun Chhay"
date: "7 September 2021"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Lets first load a few packages which we will use for this analysis:

```{r Load Packages}
#install.packages("randomForest")
#install.packages("caret")
library(randomForest)
library(caret)
library(xgboost)
library(pROC)
```

## Random Forests

The random forest algorithm works by building bootstrapped trees using only a selection of variables to split the data at each node. 

The tuning parameters we can use for random forests are:

* Number of trees - The number of trees we build in the model
* mtry - The number of variables tried at each split in the model
* nodesize - The minimum size of the terminal nodes (Default 5)

Some other parameters that can affect the model are:

* sampsize - The sizes of the bootstrap sample to take (Can be used for imbalanced data by specifying the number of samples from each class to use.)
* maxnodes - The maximum number of terminal nodes in the model (This is generally controlled for using the nodesize parameter)
* replace - Should the sampled datasets used for each tree be taken with replacement for the data.(Answer for this is yes unless only using a subsample of the data)


In general random forests will not overfit our data. Therefore we can build a large number of trees and focus on tuning the mtry and node size parameters. Which will allow us to visualize the parameter combinations and how they perform. 

## TOR Data

For this analysis we are going to be analyzing internet connections to predict if they come from the dark web. 

Darknet is the unused address space of the internet which is not speculated to interact with other computers in the world. Any communication from the dark space is considered skeptical owing to its passive listening nature which accepts incoming packets, but outgoing packets are not supported. Due to the absence of legitimate hosts in the darknet, any traffic is contemplated to be unsought and is characteristically treated as probe, backscatter, or misconfiguration. Darknets are also known as network telescopes, sinkholes, or blackholes.

To access the Darknet or Darkweb people will generally use the TOR browser:

"Back in the mid-'90s, when the US Navy was looking into ways to securely communicate sensitive intelligence information, a mathematician and two computer scientists emerged from the Naval Research Lab with something called "onion routing." It was a new kind of technology that would protect your internet traffic with layers of privacy. By 2003, The Onion Routing project, acronymed Tor, was in the hands of the public, where its vast network of users -- the engine enabling Tor -- has since continued to grow. 

Today, thousands of volunteers all over the world are connecting their computers to the internet to create the Tor network by becoming "nodes" or "relays" for your internet traffic. 

At a basic level, Tor is a type of internet-connected network with its own internet browser. Once you connect to the internet with the Tor browser, your internet traffic is stripped of its first layer of identifying information as it enters the Tor network, and is then sent bouncing through those relay nodes, which serve to encrypt and privatize your data, layer by layer -- like an onion. Finally, your traffic hits an exit node and leaves the Tor network for the open web. 

Once you're in the Tor network, it's nearly impossible for others to track your traffic's manic pinballing path across the globe. And once you leave the Tor network via an exit node, the website you view (assuming it has HTTPS in front of its address) isn't sure which part of the world you're hailing from, offering you more privacy and protection."

This data is stored as `tor_data.rda`. Lets load the data into the work space:

```{r load tor data}
load("tor_data.rda")
```

The data is already split into training and test sets using an 80/20 split called `train_db` and `test_db` respectively. 

```{r Summary Training Data}
summary(train_db)
```

We see we have 24 variables for our analysis. These variables relate to the network connection made between the source and destination. The way internet traffic works is that data is broken up into packets and sent from the source to the destination which then sends packets back to the source. A flow becomes inactive after no packets have been observed for a period of time, this value is usually 15 seconds. The variables we have are:

* Flow.Duration - A flow refers to any connection or connection-like communication channel. The duration measures the length of time between the first and last packets sent. 
* Flow.Bytes.s - Number of bytes sent in the connection
* Flow.Packets.s - Number of packets sent in the communication
* Flow.IAT.Mean - Packets flow inter arrival time Mean
* Flow.IAT.Std - Packets flow inter arrival time Standard deviation
* Flow.IAT.Max - Packets flow inter arrival time Max.
* Flow.IAT.Min - Packets flow inter arrival time Min
* Fwd.IAT.Mean - Forward   inter   arrival   time,   the   time between    two    packets    Sent    forward direction Mean
* Fwd.IAT.Std - Forward   inter   arrival   time,   the   time between    two    packets    sent    forward direction Standard deviation.
* Fwd.IAT.Max - Forward   inter   arrival   time,   the time between    two    packets    sent    forward direction Max
* Fwd.IAT.Min - Forward   inter   arrival   time,   the   time between    two    packets    sent    forward direction Min.
* Bwd.IAT.Mean -Backward   inter   arrival   time,   the   time between    two    packets    sent    backward Mean.
* Bwd.IAT.Std - Backward   inter   arrival   time,   the   time between    two    packets    sent    backward Standard deviation.
* Bwd.IAT.Max - Backward   inter   arrival   time,   the   time between two packets sent backward Max.
* Bwd.IAT.Min - Backward   inter   arrival   time,   the   time between two packets sent backward Min
* Active.Mean - The  amount  of  time  a  flow  was  active before becoming idle mean.
* Active.Std - The  amount  of  time  a  flow  was  active before becoming idle Standard deviation
* Active.Max - The  amount  of  time  a  flow  was  active before becoming idle Max.    
* Active.Min - The  amount  of  time  a  flow  was  active before becoming idle Min.
* Idle.Mean - The   amount   of   time   a   flow   was   idle before becoming active Mean
* Idle.Std - The   amount   of   time   a   flow   was   idle before becoming active Std deviation.
* Idle.Max - The   amount   of   time   a   flow   was   idle before becoming active Max.
* Idle.Min - The   amount   of   time   a   flow   was   idle before becoming active Min.
* label - Either TOR indicating a TOR connection or nonTOR indication a non-TOR connection

We will use the label as a response variable and the other variables as explanatory variables.


# Best Random Forest

It seems like the best set of parameters for this tree are mtry 12 and node size 1.


```{r}
rf_mod <- randomForest(label ~., # Set tree formula
                         data = train_db, # Set dataset
                         ntree = 200,
                         nodesize = 1,
                         mtry = 12) # Set number of trees to use
rf_preds <- predict(rf_mod, test_db, type = "prob") # Create predictions for random forest model

# Convert predictions to classes, using 0.5
rf_pred_class <- rep("nonTOR", nrow(rf_preds))
rf_pred_class[rf_preds[,2] >= 0.5] <- "TOR"

t <- table(rf_pred_class, test_db$label) # Create table
confusionMatrix(t, positive = "TOR") # Produce confusion matrix
```


## Assignment -  20 Total Marks

* Apply a bagging model to the DarkNet dataset (2 marks)

```{r}
set.seed(258506) 

# Use random forest to do bagging
bag_mod <- randomForest(label ~.,
                data = train_db, 
                mtry = 24,
                ntree = 200)
bag_mod
bag_preds <- predict(bag_mod, test_db, type ="prob")

```


* Apply an XGBoost model to the DarkNet dataset (2 marks)

```{r xgboost prep}
dtrain <- xgb.DMatrix(data = as.matrix(train_db[, 1:23]), label = as.numeric(train_db$label) -1)
# Create test matrix
dtest <- xgb.DMatrix(data = as.matrix(test_db[, 1:23]), label = as.numeric(test_db$label) - 1)
```


```{r}
set.seed(1123111)
bst_1 <- xgboost(data = dtrain, 
               nrounds = 100,
               verbose = 1, 
                print_every_n = 20, 
               objective = "binary:logistic", 
               eval_metric = "auc",
               eval_metric = "error")

```


* Visualize and decide the optimal number of iterations for XGBoost.(Plot the error curve against the number of iterations) (2 marks)

```{r}
bst <- xgb.cv(data = dtrain, 
              nfold = 5, 
               nrounds = 250, 
               early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 1, 
               print_every_n = 20, 
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error")

g_6 <- ggplot(bst$evaluation_log, aes(x = iter, y = test_error_mean))+
  geom_point(alpha = 0.5) +
  theme_bw() + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank()) + 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")
g_6


```


* Tune the eta parameter for XGboost (2 marks)

```{r}
set.seed(121111)
bst_mod_1 <-  xgb.cv(data = dtrain, 
              nfold = 5, 
              eta = 0.3, 
              nrounds = 250, 
              early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 1, 
               print_every_n = 20, 
               objective = "binary:logistic",
               eval_metric = "auc",
               eval_metric = "error")

set.seed(121111)
bst_mod_2 <-  xgb.cv(data = dtrain, 
              nfold = 5, 
              eta = 0.1, 
              nrounds = 250, 
              early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 1, 
               print_every_n = 20,
               objective = "binary:logistic", 
               eval_metric = "auc",
               eval_metric = "error")

set.seed(121111)
bst_mod_3 <- xgb.cv(data = dtrain, 
              nfold = 5, 
              eta = 0.05, 
              nrounds = 250, 
              early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 1, 
               print_every_n = 20,
               objective = "binary:logistic", 
               eval_metric = "auc",
               eval_metric = "error")

set.seed(121111)
bst_mod_4 <- xgb.cv(data = dtrain, 
              nfold = 5, 
              eta = 0.01, 
              nrounds = 250,
              early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 1, 
               print_every_n = 20, 
               objective = "binary:logistic", 
               eval_metric = "auc",
               eval_metric = "error")

set.seed(121111)
bst_mod_5 <- xgb.cv(data = dtrain, 
              nfold = 5, 
              eta = 0.005, 
              nrounds = 250, 
              early_stopping_rounds = 50, 
               verbose = 1, 
               nthread = 1, 
               print_every_n = 20, 
               objective = "binary:logistic",
               eval_metric = "auc",
               eval_metric = "error")

# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_error_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_error_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_error_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_error_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_error_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)

# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank()) + 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        panel.border = element_blank(), 
        panel.background = element_blank()) + 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  
g_7

#Select Best Model
bst_final <- xgboost(data = dtrain, 
              eta = 0.3, 
              nrounds = 75, 
              early_stopping_rounds = 25, 
              verbose = 1, 
              nthread = 1, 
              print_every_n = 20, 
              objective = "binary:logistic", 
              eval_metric = "auc",
              eval_metric = "error")

boost_preds <- predict(bst_final, dtest)
pred_dat <- cbind.data.frame(boost_preds , test_db$label)
boost_pred_class <- rep("nonTOR", length(boost_preds))
boost_pred_class[boost_preds >= 0.5] <- "TOR"

```


* Extract and plot the variable importance for XGBoost (1 mark)

```{r}
# Extract importance
imp_mat <- xgb.importance(model = bst_final)

# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 10)
```


* Which features were most important for the XGBoost model? (1 mark)

The most important features for the XGBoost model are Bwd.iAT.Std, Flow.Bytes.s and Flow.Duration. 

* Compare the three models (Last random forest from pre-assignment, bagging, XGBoost) using an ROC plot. (2 marks)

```{r}
rf_preds_tor <- rf_preds[, "TOR"]
bag_preds_tor <- bag_preds[, "TOR"]

roc1 = roc(test_db$label, rf_preds_tor)

roc2 = roc(test_db$label, bag_preds_tor)

roc3 = roc(test_db$label, boost_preds)

plot.roc(roc1, print.auc = TRUE, col = "red", print.auc.col = "red")

plot.roc(roc2, print.auc = TRUE, col = "blue", print.auc.col = "blue")

plot.roc(roc3, print.auc = TRUE, col ="green", print.auc.col = "green")

summary(as.factor(train_db$label))

```


* Which of the three models (random forest, bagging, XGBoost) gave the best results? (1 mark)

Judging from the AUC between the three models, XGBoost gave the best results. 

* Can you beat a sensitivity score of 0.96 while keeping overall accuracy above 0.98 and the cut-off set as 0.5? (4 marks - Partial Credit for Attempt) 

```{r}
zero_weight <- 47828/6436
#Select Best Model
bst_final_mod <- xgboost(data = dtrain,
              eta = 0.3,
              nrounds = 75, 
              early_stopping_rounds = 25, 
              verbose = 1, 
              nthread = 1, 
              print_every_n = 20,
              scale_pos_weight = zero_weight,
              objective = "binary:logistic",
              eval_metric = "auc",
              eval_metric = "error")

boost_preds_mod <- predict(bst_final_mod, dtest)
pred_dat <- cbind.data.frame(boost_preds_mod , test_db$label)# Convert predictions to classes, using optimal cut-off
boost_pred_class_mod <- rep("nonTOR", length(boost_preds_mod))
boost_pred_class_mod[boost_preds_mod >= 0.5] <- "TOR"

t <- table(boost_pred_class_mod, test_db$label) # Create table
confusionMatrix(t, positive = "TOR")

```


3 marks for analysis decisions, modeling decisions and code readability/usability.



